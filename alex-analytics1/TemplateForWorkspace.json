{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "alex-analytics1"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"AzureSearch1_key": {
			"type": "secureString",
			"metadata": "Secure string for 'key' of 'AzureSearch1'"
		},
		"AzureTableStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureTableStorage1'"
		},
		"alex-analytics1-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'alex-analytics1-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:alex-analytics1.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureSearch1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://cpsc-recalls-ai-search.search.windows.net"
		},
		"alex-analytics1-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://alexanalytics1.dfs.core.windows.net"
		},
		"dthslinkmi_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "dths-test-sqlserver.database.windows.net"
		},
		"dthslinkmi_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "dths"
		},
		"Trigger_9wq_properties_CopyStatusToParquet_parameters_windowStart": {
			"type": "string",
			"defaultValue": "@trigger().scheduledTime"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/CopyStatusToParquet')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy_9wq",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "statusHistoryTable"
							},
							{
								"name": "Destination",
								"value": "synapse1/statusUrls/@{formatDateTime(pipeline().parameters.windowStart,'yy')}@{formatDateTime(pipeline().parameters.windowStart,'MM')}@{formatDateTime(pipeline().parameters.windowStart,'dd')}@{formatDateTime(pipeline().parameters.windowStart,'HH')}@{formatDateTime(pipeline().parameters.windowStart,'mm')}/statuses"
							}
						],
						"typeProperties": {
							"source": {
								"type": "AzureTableSource",
								"azureTableSourceIgnoreTableNotFound": false
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"validateDataConsistency": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_9wq",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_9wq",
								"type": "DatasetReference",
								"parameters": {
									"cw_folderPath": "statusUrls/@{formatDateTime(pipeline().parameters.windowStart,'yy')}@{formatDateTime(pipeline().parameters.windowStart,'MM')}@{formatDateTime(pipeline().parameters.windowStart,'dd')}@{formatDateTime(pipeline().parameters.windowStart,'HH')}@{formatDateTime(pipeline().parameters.windowStart,'mm')}"
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"windowStart": {
						"type": "String"
					}
				},
				"annotations": [],
				"lastPublishTime": "2025-11-16T14:55:52Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/SourceDataset_9wq')]",
				"[concat(variables('workspaceId'), '/datasets/DestinationDataset_9wq')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CopyStorageToSearch')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy_o7g",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "statusHistoryTable"
							},
							{
								"name": "Destination",
								"value": "cpsc-recalls-10102024"
							}
						],
						"typeProperties": {
							"source": {
								"type": "AzureTableSource",
								"azureTableSourceIgnoreTableNotFound": false
							},
							"sink": {
								"type": "AzureSearchIndexSink",
								"writeBatchSize": 1000,
								"writeBehavior": "merge"
							},
							"enableStaging": false,
							"validateDataConsistency": false
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_o7g",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_o7g",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2025-07-29T16:32:18Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/SourceDataset_o7g')]",
				"[concat(variables('workspaceId'), '/datasets/DestinationDataset_o7g')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvToSqlPipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Run_PySpark_Notebook",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "csvToSqlPySparkNotebook",
								"type": "NotebookReference"
							},
							"sparkPool": {
								"referenceName": "SampleSpark",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true
							},
							"driverSize": "Small",
							"authentication": {
								"type": "MSI"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"fileName": {
						"type": "String",
						"defaultValue": "Florida-10102025.csv"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/csvToSqlPySparkNotebook')]",
				"[concat(variables('workspaceId'), '/bigDataPools/SampleSpark')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_9wq')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_folderPath": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "statuses",
						"folderPath": {
							"value": "@dataset().cw_folderPath",
							"type": "Expression"
						},
						"container": "synapse1"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_o7g')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSearch1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSearchIndex",
				"schema": [],
				"typeProperties": {
					"indexName": "cpsc-recalls-10102024"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSearch1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_9wq')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureTableStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureTable",
				"schema": [],
				"typeProperties": {
					"tableName": "statusHistoryTable"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureTableStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_o7g')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureTableStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureTable",
				"schema": [],
				"typeProperties": {
					"tableName": "statusHistoryTable"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureTableStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvtosql')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "alex-analytics1-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "synapse/workspaces/alex-analytics1",
						"fileSystem": "synapse1"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/alex-analytics1-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqldths')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "dthslinkmi",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "batchId",
						"type": "int",
						"precision": 10
					},
					{
						"name": "state",
						"type": "varchar"
					},
					{
						"name": "stateFIPS",
						"type": "char"
					},
					{
						"name": "year",
						"type": "int",
						"precision": 10
					},
					{
						"name": "dateReceived",
						"type": "date"
					},
					{
						"name": "dateReady",
						"type": "date"
					},
					{
						"name": "totalRecords",
						"type": "int",
						"precision": 10
					},
					{
						"name": "toReview",
						"type": "int",
						"precision": 10
					},
					{
						"name": "reviewedCount",
						"type": "int",
						"precision": 10
					},
					{
						"name": "awaitingCount",
						"type": "int",
						"precision": 10
					},
					{
						"name": "dthsCount",
						"type": "int",
						"precision": 10
					},
					{
						"name": "abdtCount",
						"type": "int",
						"precision": 10
					},
					{
						"name": "rejectedCount",
						"type": "int",
						"precision": 10
					},
					{
						"name": "batchStatus",
						"type": "varchar"
					},
					{
						"name": "submittedBy",
						"type": "varchar"
					},
					{
						"name": "submittedDate",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "createdBy",
						"type": "varchar"
					},
					{
						"name": "createdDate",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "modifiedBy",
						"type": "varchar"
					},
					{
						"name": "modifiedDate",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Batches"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dthslinkmi')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSearch1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSearch",
				"typeProperties": {
					"url": "[parameters('AzureSearch1_properties_typeProperties_url')]",
					"key": {
						"type": "SecureString",
						"value": "[parameters('AzureSearch1_key')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureTableStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureTableStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureTableStorage1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/alex-analytics1-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('alex-analytics1-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/alex-analytics1-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('alex-analytics1-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dthslinkmi')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "linked service to dths db with managed identity",
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"server": "[parameters('dthslinkmi_properties_typeProperties_server')]",
					"database": "[parameters('dthslinkmi_properties_typeProperties_database')]",
					"encrypt": "mandatory",
					"trustServerCertificate": false,
					"authenticationType": "SystemAssignedManagedIdentity"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger_9wq')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "CopyStatusToParquet",
							"type": "PipelineReference"
						},
						"parameters": {
							"windowStart": "[parameters('Trigger_9wq_properties_CopyStatusToParquet_parameters_windowStart')]"
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 24,
						"startTime": "2025-07-29T16:33:00Z",
						"endTime": "2025-08-31T16:33:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/CopyStatusToParquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger_o7g')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "CopyStorageToSearch",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 4,
						"startTime": "2025-07-29T16:22:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/CopyStorageToSearch')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlIntegration')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 10,
							"cleanup": false,
							"customProperties": []
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvtodthssql')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "csvtosql",
								"type": "DatasetReference"
							},
							"name": "SourceCSV"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "sqldths",
								"type": "DatasetReference"
							},
							"name": "SinkBatches"
						}
					],
					"transformations": [
						{
							"name": "distinct"
						}
					],
					"script": "source(output(\n\t\tbatchId as integer,\n\t\tstate as string,\n\t\tstateFIPS as string,\n\t\tyear as integer,\n\t\tdateReceived as date,\n\t\tdateReady as date,\n\t\ttotalRecords as integer,\n\t\ttoReview as integer,\n\t\treviewedCount as integer,\n\t\tawaitingCount as integer,\n\t\tdthsCount as integer,\n\t\tabdtCount as integer,\n\t\trejectedCount as integer,\n\t\tbatchStatus as string,\n\t\tsubmittedBy as string,\n\t\tsubmittedDate as timestamp,\n\t\tcreatedBy as string,\n\t\tcreatedDate as timestamp,\n\t\tmodifiedBy as string,\n\t\tmodifiedDate as timestamp,\n\t\trecordId as integer,\n\t\tcpscdocnum as string,\n\t\tpdf as string,\n\t\tfirstName as string,\n\t\tlastName as string,\n\t\tnewage as integer,\n\t\tgender as string,\n\t\thispanic as string,\n\t\tethnicity as string,\n\t\trace as string,\n\t\tdod as date,\n\t\tidate as date,\n\t\tbdate as date,\n\t\taddr as string,\n\t\trescsz as string,\n\t\trescnty as string,\n\t\tinjplace as string,\n\t\tinjcsz as string,\n\t\tinjcounty as string,\n\t\tdthcsz as string,\n\t\tdthcnty as string,\n\t\twork as string,\n\t\tautop as string,\n\t\tcpsc_manner as string,\n\t\tcpsc_injdesc as string,\n\t\tcpsc_cod1 as string,\n\t\tcpsc_cod2 as string,\n\t\tcpsc_cod3 as string,\n\t\tcpsc_cod4 as string,\n\t\tcpsc_othcond as string,\n\t\tproduct1 as string,\n\t\tbertprob1 as float,\n\t\tproduct2 as string,\n\t\tbertprob2 as float,\n\t\tproduct3 as string,\n\t\tbertprob3 as float,\n\t\tscope as string,\n\t\tscopebertprob as float,\n\t\tcpsc_cod as string,\n\t\tcodprob as float,\n\t\tfireInvolved as string,\n\t\tfireprob as float,\n\t\tflag as string,\n\t\treviewed as string,\n\t\treviewedBy as string,\n\t\treviewedDate as timestamp,\n\t\treviewNotes as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> SourceCSV\nSourceCSV select(skipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: false) ~> distinct\ndistinct sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tinput(\n\t\tbatchId as integer,\n\t\tstate as string,\n\t\tstateFIPS as string,\n\t\tyear as integer,\n\t\tdateReceived as date,\n\t\tdateReady as date,\n\t\ttotalRecords as integer,\n\t\ttoReview as integer,\n\t\treviewedCount as integer,\n\t\tawaitingCount as integer,\n\t\tdthsCount as integer,\n\t\tabdtCount as integer,\n\t\trejectedCount as integer,\n\t\tbatchStatus as string,\n\t\tsubmittedBy as string,\n\t\tsubmittedDate as timestamp,\n\t\tcreatedBy as string,\n\t\tcreatedDate as timestamp,\n\t\tmodifiedBy as string,\n\t\tmodifiedDate as timestamp\n\t),\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tformat: 'table',\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true,\n\tsaveOrder: 0,\n\terrorHandlingOption: 'stopOnFirstError') ~> SinkBatches"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/csvtosql')]",
				"[concat(variables('workspaceId'), '/datasets/sqldths')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvToSqlPySparkNotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SampleSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": true,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3c80b1fb-bd66-4708-8bcc-cc3dfb47c62a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/75bad4c0-165f-4c86-971d-364c6e6ce1dd/resourceGroups/rg-analytics-dev/providers/Microsoft.Synapse/workspaces/alex-analytics1/bigDataPools/SampleSpark",
						"name": "SampleSpark",
						"type": "Spark",
						"endpoint": "https://alex-analytics1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import os\n",
							"# === Parameters from pipeline ===\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"import os\n",
							"# === Parameters from pipeline ===\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"\n",
							"# --------------------------\n",
							"# Manual defaults for testing\n",
							"# --------------------------\n",
							"# Python notebook code\n",
							"file_name  = \"Florida-10102025.csv\"\n",
							"state_name = \"FL\"\n",
							"file_date  = \"20251010\"\n",
							"print(f\"File: {file_name}, State: {state_name}, Date: {file_date}\")\n",
							"\n",
							"\n",
							"if file_name is None:\n",
							"    print(f\"invalid file name or path\")\n",
							"    raise ValueError(\"Parameter 'file_name' is required.\")\n",
							"\n",
							"# Optional for JDBC URL if using environment variables or pipeline\n",
							"# dbutils.widgets.text(\"jdbcUrl\", \"\")\n",
							"# dbutils.widgets.text(\"targetTable\", \"\")\n",
							"\n",
							"#targetDB\n",
							"\n",
							"\n",
							"#jdbc_url     = os.environ.get(\"SQL_JDBC_URL\")\n",
							"#target_table = os.environ.get(\"SQL_TABLE\")\n",
							"# === ADLS configuration ===\n",
							"container = \"synapse1\"\n",
							"account = \"alexanalytics1\"  # without .dfs.core.windows.net\n",
							"folder = \"synapse/workspaces/alex-analytics1\"\n",
							"file_path = f\"abfss://{container}@{account}.dfs.core.windows.net/{folder}/{file_name}\"\n",
							"\n",
							"# === Read CSV with schema inferred ===\n",
							"df = (spark.read\n",
							"      .option(\"header\", \"true\")\n",
							"      .option(\"inferSchema\", \"true\")\n",
							"      .option(\"quote\", '\"')\n",
							"      .option(\"escape\", '\\\\')\n",
							"      .csv(file_path))\n",
							"print(\"Read CSV File\")\n",
							"df.show()\n",
							"\n",
							"\n",
							"\n",
							"# === Simple transformation: trim all string columns ===\n",
							"for col in df.columns:\n",
							"    df = df.withColumn(col, trim(df[col]))\n",
							"\n",
							"# === SQL Server connection ===\n",
							"jdbc_url = \"jdbc:sqlserver://dths-test-sqlserver.database.windows.net:1433;databaseName=dths\"\n",
							"target_table = \"dbo.batches\"\n",
							"\n",
							"# Get token using notebook's Managed Identity (automatic)\n",
							"token = mssparkutils.credentials.getToken('dw')\n",
							"\n",
							"connectionProperties = {\n",
							"    \"accessToken\": token,\n",
							"    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
							"}\n",
							"# === Write to SQL Server efficiently ===\n",
							"# Replace \"BatchID\" since it's the INSERT_IDENTITY column\n",
							"df_to_write = df.drop(\"BatchID\")\n",
							"\n",
							"# Then write\n",
							"(df_to_write.write\n",
							"   .format(\"jdbc\")\n",
							"   .mode(\"append\")\n",
							"   .option(\"url\", jdbc_url)\n",
							"   .option(\"dbtable\", \"dbo.batches\")\n",
							"   .option(\"batchsize\", \"10000\")\n",
							"   .options(**connectionProperties)\n",
							"   .save())\n",
							"\n",
							"print(f\"Data successfully written to {target_table}\")\n",
							"\n",
							"# === Archive processed file ===\n",
							"source_path = f\"{folder}/{file_name}\"\n",
							"archive_path = f\"{folder}/archive/{file_name}\"\n",
							"mssparkutils.fs.mkdirs(f\"abfss://{container}@{account}.dfs.core.windows.net/{folder}/archive\")\n",
							"mssparkutils.fs.mv(\n",
							"    f\"abfss://{container}@{account}.dfs.core.windows.net/{source_path}\",\n",
							"    f\"abfss://{container}@{account}.dfs.core.windows.net/{archive_path}\"\n",
							")\n",
							"print(f\"Moved file to archive: {archive_path}\")\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"# Get token using notebook's Managed Identity (automatic)\n",
							"token = mssparkutils.credentials.getToken('dw')\n",
							"\n",
							"# JDBC connection to Azure SQL\n",
							"server = \"dths-test-sqlserver.database.windows.net\"\n",
							"database = \"dths\"\n",
							"table = \"dbo.batches\"\n",
							"\n",
							"jdbcUrl = f\"jdbc:sqlserver://{server}:1433;database={database}\";\n",
							"\n",
							"connectionProperties = {\n",
							"    \"accessToken\": token,\n",
							"    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
							"}\n",
							"\n",
							"df = spark.read.jdbc(url=jdbcUrl, table=table, properties=connectionProperties)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvToSqlPySparkNotebook_521b6b5d-230a-46c1-89b6-ae13a77bf199')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SampleSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": true,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "51bce5bf-8a75-4c7a-8cf5-351f5cfc051a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/75bad4c0-165f-4c86-971d-364c6e6ce1dd/resourceGroups/rg-analytics-dev/providers/Microsoft.Synapse/workspaces/alex-analytics1/bigDataPools/SampleSpark",
						"name": "SampleSpark",
						"type": "Spark",
						"endpoint": "https://alex-analytics1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import os\n",
							"# === Parameters from pipeline ===\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"\n",
							"# --------------------------\n",
							"# Manual defaults for testing\n",
							"# --------------------------\n",
							"# Python notebook code\n",
							"file_name  = globals().get(\"fileName\")\n",
							"state_name = \"FL\"\n",
							"file_date  = \"20251010\"\n",
							"print(f\"File: {file_name}, State: {state_name}, Date: {file_date}\")\n",
							"\n",
							"\n",
							"if file_name is None:\n",
							"    print(f\"invalid file name or path\")\n",
							"    raise ValueError(\"Parameter 'file_name' is required.\")\n",
							"\n",
							"# Optional for JDBC URL if using environment variables or pipeline\n",
							"# dbutils.widgets.text(\"jdbcUrl\", \"\")\n",
							"# dbutils.widgets.text(\"targetTable\", \"\")\n",
							"\n",
							"#targetDB\n",
							"\n",
							"\n",
							"#jdbc_url     = os.environ.get(\"SQL_JDBC_URL\")\n",
							"#target_table = os.environ.get(\"SQL_TABLE\")\n",
							"# === ADLS configuration ===\n",
							"container = \"synapse1\"\n",
							"account = \"alexanalytics1\"  # without .dfs.core.windows.net\n",
							"folder = \"synapse/workspaces/alex-analytics1\"\n",
							"file_path = f\"abfss://{container}@{account}.dfs.core.windows.net/{folder}/{file_name}\"\n",
							"\n",
							"# === Read CSV with schema inferred ===\n",
							"df = (spark.read\n",
							"      .option(\"header\", \"true\")\n",
							"      .option(\"inferSchema\", \"true\")\n",
							"      .option(\"quote\", '\"')\n",
							"      .option(\"escape\", '\\\\')\n",
							"      .csv(file_path))\n",
							"print(\"Read CSV File\")\n",
							"df.show()\n",
							"\n",
							"# === Add metadata columns ===\n",
							"from pyspark.sql.functions import lit, current_timestamp, trim\n",
							"df = (df\n",
							"      .withColumn(\"SourceState\", lit(state_name))\n",
							"      .withColumn(\"SourceFileDate\", lit(file_date))\n",
							"      .withColumn(\"IngestionTimestamp\", current_timestamp()))\n",
							"\n",
							"# === Simple transformation: trim all string columns ===\n",
							"for col in df.columns:\n",
							"    df = df.withColumn(col, trim(df[col]))\n",
							"\n",
							"# === SQL Server connection ===\n",
							"jdbc_url = \"jdbc:sqlserver://dths-test-sqlserver.database.windows.net:1433;databaseName=dths;encrypt=true;trustServerCertificate=false;authentication=ActiveDirectoryMsi\"\n",
							"target_table = \"dbo.batches\"\n",
							"\n",
							"# === Write to SQL Server efficiently ===\n",
							"(df.write\n",
							"   .format(\"jdbc\")\n",
							"   .mode(\"append\")\n",
							"   .option(\"url\", jdbc_url)\n",
							"   .option(\"dbtable\", target_table)\n",
							"   .option(\"batchsize\", 5000)\n",
							"   .save())\n",
							"\n",
							"print(f\"Data successfully written to {target_table}\")\n",
							"\n",
							"# === Archive processed file ===\n",
							"source_path = f\"{folder}/{file_name}\"\n",
							"archive_path = f\"{folder}/archive/{file_name}\"\n",
							"mssparkutils.fs.mkdirs(f\"abfss://{container}@{account}.dfs.core.windows.net/{folder}/archive\")\n",
							"mssparkutils.fs.mv(\n",
							"    f\"abfss://{container}@{account}.dfs.core.windows.net/{source_path}\",\n",
							"    f\"abfss://{container}@{account}.dfs.core.windows.net/{archive_path}\"\n",
							")\n",
							"print(f\"Moved file to archive: {archive_path}\")\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvToSqlPySparkNotebook_b32150b8-f187-4ce8-a31c-94d1bf6f2ba9')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SampleSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": true,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7c15b048-d168-4dd5-818d-fc559efe01d6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/75bad4c0-165f-4c86-971d-364c6e6ce1dd/resourceGroups/rg-analytics-dev/providers/Microsoft.Synapse/workspaces/alex-analytics1/bigDataPools/SampleSpark",
						"name": "SampleSpark",
						"type": "Spark",
						"endpoint": "https://alex-analytics1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import os\n",
							"# === Parameters from pipeline ===\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"import os\n",
							"# === Parameters from pipeline ===\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"\n",
							"# --------------------------\n",
							"# Manual defaults for testing\n",
							"# --------------------------\n",
							"# Python notebook code\n",
							"file_name = mssparkutils.runtime.context.get('file_name', 'Florida-10102025.csv')\n",
							"\n",
							"# Auto-extract state and date from filename (very common pattern)\n",
							"state_name = file_name.split('-')[0]          # \"Florida\" → you can map to \"FL\" if needed\n",
							"file_date  = file_name.split('-')[1].replace('.csv', '')  # \"10102025.csv\" → \"20251010\"\n",
							"\n",
							"# Optional: map state name to abbreviation\n",
							"state_map = {\"Florida\": \"FL\", \"California\": \"CA\", \"Texas\": \"TX\"}\n",
							"state_name = state_map.get(state_name, state_name[:2].upper())\n",
							"\n",
							"print(f\"Auto-detected → {file_name} | {state_name} | {file_date}\")\n",
							"if file_name is None:\n",
							"    print(f\"invalid file name or path\")\n",
							"    raise ValueError(\"Parameter 'file_name' is required.\")\n",
							"\n",
							"# Optional for JDBC URL if using environment variables or pipeline\n",
							"# dbutils.widgets.text(\"jdbcUrl\", \"\")\n",
							"# dbutils.widgets.text(\"targetTable\", \"\")\n",
							"\n",
							"#targetDB\n",
							"\n",
							"\n",
							"#jdbc_url     = os.environ.get(\"SQL_JDBC_URL\")\n",
							"#target_table = os.environ.get(\"SQL_TABLE\")\n",
							"# === ADLS configuration ===\n",
							"container = \"synapse1\"\n",
							"account = \"alexanalytics1\"  # without .dfs.core.windows.net\n",
							"folder = \"synapse/workspaces/alex-analytics1\"\n",
							"file_path = f\"abfss://{container}@{account}.dfs.core.windows.net/{folder}/{file_name}\"\n",
							"\n",
							"# === Read CSV with schema inferred ===\n",
							"df = (spark.read\n",
							"      .option(\"header\", \"true\")\n",
							"      .option(\"inferSchema\", \"true\")\n",
							"      .option(\"quote\", '\"')\n",
							"      .option(\"escape\", '\\\\')\n",
							"      .csv(file_path))\n",
							"print(\"Read CSV File\")\n",
							"df.show()\n",
							"\n",
							"\n",
							"\n",
							"# === Simple transformation: trim all string columns ===\n",
							"for col in df.columns:\n",
							"    df = df.withColumn(col, trim(df[col]))\n",
							"\n",
							"# === SQL Server connection ===\n",
							"jdbc_url = \"jdbc:sqlserver://dths-test-sqlserver.database.windows.net:1433;databaseName=dths\"\n",
							"target_table = \"dbo.batches\"\n",
							"\n",
							"# Get token using notebook's Managed Identity (automatic)\n",
							"token = mssparkutils.credentials.getToken('dw')\n",
							"\n",
							"connectionProperties = {\n",
							"    \"accessToken\": token,\n",
							"    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
							"}\n",
							"# === Write to SQL Server efficiently ===\n",
							"# Replace \"BatchID\" since it's the INSERT_IDENTITY column\n",
							"df_to_write = df.drop(\"BatchID\")\n",
							"\n",
							"# Then write\n",
							"(df_to_write.write\n",
							"   .format(\"jdbc\")\n",
							"   .mode(\"append\")\n",
							"   .option(\"url\", jdbc_url)\n",
							"   .option(\"dbtable\", \"dbo.batches\")\n",
							"   .option(\"batchsize\", \"10000\")\n",
							"   .options(**connectionProperties)\n",
							"   .save())\n",
							"\n",
							"print(f\"Data successfully written to {target_table}\")\n",
							"\n",
							"# === Archive processed file ===\n",
							"source_path = f\"{folder}/{file_name}\"\n",
							"archive_path = f\"{folder}/archive/{file_name}\"\n",
							"mssparkutils.fs.mkdirs(f\"abfss://{container}@{account}.dfs.core.windows.net/{folder}/archive\")\n",
							"mssparkutils.fs.mv(\n",
							"    f\"abfss://{container}@{account}.dfs.core.windows.net/{source_path}\",\n",
							"    f\"abfss://{container}@{account}.dfs.core.windows.net/{archive_path}\"\n",
							")\n",
							"print(f\"Moved file to archive: {archive_path}\")\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"# Get token using notebook's Managed Identity (automatic)\n",
							"token = mssparkutils.credentials.getToken('dw')\n",
							"\n",
							"# JDBC connection to Azure SQL\n",
							"server = \"dths-test-sqlserver.database.windows.net\"\n",
							"database = \"dths\"\n",
							"table = \"dbo.batches\"\n",
							"\n",
							"jdbcUrl = f\"jdbc:sqlserver://{server}:1433;database={database}\";\n",
							"\n",
							"connectionProperties = {\n",
							"    \"accessToken\": token,\n",
							"    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
							"}\n",
							"\n",
							"df = spark.read.jdbc(url=jdbcUrl, table=table, properties=connectionProperties)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvToSqlPySparkNotebook_b32150b8-f187-4ce8-a31c-94d1bf6f2ba9_sd4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SampleSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": true,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4620e6ab-a895-4149-b9ab-67c7553f8d2d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/75bad4c0-165f-4c86-971d-364c6e6ce1dd/resourceGroups/rg-analytics-dev/providers/Microsoft.Synapse/workspaces/alex-analytics1/bigDataPools/SampleSpark",
						"name": "SampleSpark",
						"type": "Spark",
						"endpoint": "https://alex-analytics1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import os\n",
							"# === Parameters from pipeline ===\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import os\n",
							"# === Parameters from pipeline ===\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"\n",
							"# --------------------------\n",
							"# Manual defaults for testing\n",
							"# --------------------------\n",
							"# Python notebook code\n",
							"file_name = mssparkutils.runtime.context.get('file_name', 'Florida-10102025.csv')\n",
							"\n",
							"# Auto-extract state and date from filename (very common pattern)\n",
							"state_name = file_name.split('-')[0]          # \"Florida\" → you can map to \"FL\" if needed\n",
							"file_date  = file_name.split('-')[1].replace('.csv', '')  # \"10102025.csv\" → \"20251010\"\n",
							"\n",
							"# Optional: map state name to abbreviation\n",
							"state_map = {\"Florida\": \"FL\", \"California\": \"CA\", \"Texas\": \"TX\"}\n",
							"state_name = state_map.get(state_name, state_name[:2].upper())\n",
							"\n",
							"print(f\"Auto-detected → {file_name} | {state_name} | {file_date}\")\n",
							"\n",
							"# Optional for JDBC URL if using environment variables or pipeline\n",
							"# dbutils.widgets.text(\"jdbcUrl\", \"\")\n",
							"# dbutils.widgets.text(\"targetTable\", \"\")\n",
							"\n",
							"#targetDB\n",
							"\n",
							"\n",
							"#jdbc_url     = os.environ.get(\"SQL_JDBC_URL\")\n",
							"#target_table = os.environ.get(\"SQL_TABLE\")\n",
							"# === ADLS configuration ===\n",
							"container = \"synapse1\"\n",
							"account = \"alexanalytics1\"  # without .dfs.core.windows.net\n",
							"folder = \"synapse/workspaces/alex-analytics1\"\n",
							"file_path = f\"abfss://{container}@{account}.dfs.core.windows.net/{folder}/{file_name}\"\n",
							"\n",
							"# === Read CSV with schema inferred ===\n",
							"df = (spark.read\n",
							"      .option(\"header\", \"true\")\n",
							"      .option(\"inferSchema\", \"true\")\n",
							"      .option(\"quote\", '\"')\n",
							"      .option(\"escape\", '\\\\')\n",
							"      .csv(file_path))\n",
							"print(\"Read CSV File\")\n",
							"df.show()\n",
							"\n",
							"\n",
							"\n",
							"# === SQL Server connection ===\n",
							"jdbc_url = \"jdbc:sqlserver://dths-test-sqlserver.database.windows.net:1433;databaseName=dths\"\n",
							"target_table = \"dbo.batches\"\n",
							"\n",
							"# Get token using notebook's Managed Identity (automatic)\n",
							"token = mssparkutils.credentials.getToken('dw')\n",
							"\n",
							"connectionProperties = {\n",
							"    \"accessToken\": token,\n",
							"    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
							"}\n",
							"# === Write to SQL Server efficiently ===\n",
							"# Replace \"BatchID\" since it's the INSERT_IDENTITY column\n",
							"df_to_write = df.drop(\"BatchID\")\n",
							"\n",
							"# Then write\n",
							"(df_to_write.write\n",
							"   .format(\"jdbc\")\n",
							"   .mode(\"append\")\n",
							"   .option(\"url\", jdbc_url)\n",
							"   .option(\"dbtable\", \"dbo.batches\")\n",
							"   .option(\"batchsize\", \"10000\")\n",
							"   .options(**connectionProperties)\n",
							"   .save())\n",
							"\n",
							"print(f\"Data successfully written to {target_table}\")\n",
							"\n",
							"# === Archive processed file ===\n",
							"source_path = f\"{folder}/{file_name}\"\n",
							"archive_path = f\"{folder}/archive/{file_name}\"\n",
							"mssparkutils.fs.mkdirs(f\"abfss://{container}@{account}.dfs.core.windows.net/{folder}/archive\")\n",
							"mssparkutils.fs.mv(\n",
							"    f\"abfss://{container}@{account}.dfs.core.windows.net/{source_path}\",\n",
							"    f\"abfss://{container}@{account}.dfs.core.windows.net/{archive_path}\"\n",
							")\n",
							"print(f\"Moved file to archive: {archive_path}\")\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Get token using notebook's Managed Identity (automatic)\n",
							"token = mssparkutils.credentials.getToken('dw')\n",
							"\n",
							"# JDBC connection to Azure SQL\n",
							"server = \"dths-test-sqlserver.database.windows.net\"\n",
							"database = \"dths\"\n",
							"table = \"dbo.batches\"\n",
							"\n",
							"jdbcUrl = f\"jdbc:sqlserver://{server}:1433;database={database}\";\n",
							"\n",
							"connectionProperties = {\n",
							"    \"accessToken\": token,\n",
							"    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
							"}\n",
							"\n",
							"df = spark.read.jdbc(url=jdbcUrl, table=table, properties=connectionProperties)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SampleSpark')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus2"
		}
	]
}